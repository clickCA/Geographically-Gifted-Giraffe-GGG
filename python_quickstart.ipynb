{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Quickstart with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/tutorials/python_quickstart\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on Google AI</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOxMUKTxR-_j"
      },
      "source": [
        "This quickstart demonstrates how to use the Python SDK for the Gemini API, which gives you access to Google's Gemini large language models. In this quickstart, you will learn how to:\n",
        "\n",
        "1. Set up your development environment and API access to use Gemini.\n",
        "2. Generate text responses from text inputs.\n",
        "3. Generate text responses from multimodal inputs (text and images).\n",
        "4. Use Gemini for multi-turn conversations (chat).\n",
        "5. Use embeddings for large language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9__zr1nSBpE"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "You can run this quickstart in [Google Colab](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb), which runs this notebook directly in the browser and does not require additional environment configuration.\n",
        "\n",
        "Alternatively, to complete this quickstart locally, ensure that your development environment meets the following requirements:\n",
        "\n",
        "-  Python 3.9+\n",
        "-  An installation of `jupyter` to run the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFPBKLapSCkM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNV1e3ASJha"
      },
      "source": [
        "### Install the Python SDK\n",
        "\n",
        "The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9OEoeosRTv-5"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCFF5VSTbcAR"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRC2HngneEeQ"
      },
      "source": [
        "Import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/click/.conda/envs/ds/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d10c38a5c91f"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "# from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYFrFPjSGNq"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHhsUxDTdw0W"
      },
      "source": [
        "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `GOOGLE_API_KEY`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmSlTHXxb5pV"
      },
      "source": [
        "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
        "\n",
        "* Put the key in the `GOOGLE_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
        "* Pass the key to `genai.configure(api_key=...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Fetch the GOOGLE_API_KEY environment variable\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ssbTMNVSMd-"
      },
      "source": [
        "## List models\n",
        "\n",
        "Now you're ready to call the Gemini API. Use `list_models` to see the available Gemini models:\n",
        "\n",
        "* `gemini-pro`: optimized for text-only prompts.\n",
        "* `gemini-pro-vision`: optimized for text-and-images prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QvvWFy08e5c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTl5NjtrhA0J"
      },
      "source": [
        "Note: For detailed information about the available models, including their capabilities and rate limits, see [Gemini models](https://ai.google.dev/models/gemini). There are options for requesting [rate limit increases](https://ai.google.dev/docs/increase_quota). The rate limit for Gemini-Pro models is 60 requests per minute (RPM).\n",
        "\n",
        "The `genai` package also supports the PaLM  family of models, but only the Gemini models support the generic, multimodal capabilities of the `generateContent` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Generate text from text inputs\n",
        "\n",
        "For text-only prompts, use the `gemini-pro` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2bcfnGEviwTI"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_2A_sxk8sK"
      },
      "source": [
        "The `generate_content` method can handle a wide variety of use cases, including multi-turn chat and multimodal input, depending on what the underlying model supports. The available models only support text and images as input, and text as output.\n",
        "\n",
        "In the simplest case, you can pass a prompt string to the <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a> method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "he-OfzBbhACQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 9.02 ms, sys: 950 Âµs, total: 9.97 ms\n",
            "Wall time: 5.21 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "place = \"Victory Monument in bangkok thailand \"\n",
        "content = f\"Travelers often lack engaging and accessible resources to learn about destinations. Traditional guidebooks are static and bulky, while online resources can be overwhelming. GGG aims to bridge this gap by offering a fun and interactive way to explore the world through a friendly chatbot.Entertainment Value:GGG's Quirky Personality: Infuse GGG's responses with humor, trivia, and unexpected facts to make learning about travel enjoyable.Can u explain {place} lore\"\n",
        "response = model.generate_content(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbrR-n_qlpFd"
      },
      "source": [
        "In simple cases, the `response.text` accessor is all you need. To display formatted Markdown text, use the `to_markdown` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "G-zBkueElVEO"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> **GGG Response:**\n",
              "> \n",
              "> Hey there, curious traveler! Buckle up and get ready for a whirlwind tour of Victory Monument, Bangkok's iconic landmark steeped in history and local lore.\n",
              "> \n",
              "> * **A Triumphant Arch:**\n",
              "> Victory Monument is a towering arch built in 1941 to commemorate Thailand's victory over French forces in the Franco-Thai War of 1940-1941. It's a proud symbol of Thai nationalism and military prowess.\n",
              "> \n",
              "> * **Spiritual Significance:**\n",
              "> Beneath the monument lies a sacred spirit house. Locals believe that paying respects here brings good luck and protection during travels. Just remember to show reverence and offer a small donation.\n",
              "> \n",
              "> * **Erawan Shrine's Protector:**\n",
              "> Victory Monument stands tall as the guardian of the famous Erawan Shrine, located about 2 kilometers away. Many Thais believe that the spirit of the Hindu god Indra resides in the statue and protects the shrine.\n",
              "> \n",
              "> * **Protest Hub:**\n",
              "> In recent years, Victory Monument has become a rallying point for political protests. It's a symbol of both national pride and the struggles for democracy in Thailand.\n",
              "> \n",
              "> * **Thrilling Nightlife:**\n",
              "> As the sun sets, the area around Victory Monument transforms into a vibrant nightlife hub. You'll find clubs, bars, and street food stalls bustling with locals and tourists alike.\n",
              "> \n",
              "> Remember, GGG's not just a chatbotâ€”I'm your buddy on this adventure! If you have any more questions about Victory Monument or anything else in Bangkok, just ask. I'll keep the fun facts and local insights flowing."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZPpoKMQoru8"
      },
      "source": [
        "If the API failed to return a result, use `GenerateContentResponse.prompt_feedback` to see if it was blocked due to safety concerns regarding the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eIQdU8AGoraT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEJupEDUo6Xj"
      },
      "source": [
        "Gemini can generate multiple possible responses for a single prompt. These possible responses are called `candidates`, and you can review them to select the most suitable one as the response.\n",
        "\n",
        "View the response candidates with <a href=\"https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentResponse#candidates\"><code>GenerateContentResponse.candidates</code></a>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QoGYz-I7o5wF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[index: 0\n",
              "content {\n",
              "  parts {\n",
              "    text: \"**GGG Response:**\\n\\nHey there, curious traveler! Buckle up and get ready for a whirlwind tour of Victory Monument, Bangkok\\'s iconic landmark steeped in history and local lore.\\n\\n* **A Triumphant Arch:**\\nVictory Monument is a towering arch built in 1941 to commemorate Thailand\\'s victory over French forces in the Franco-Thai War of 1940-1941. It\\'s a proud symbol of Thai nationalism and military prowess.\\n\\n* **Spiritual Significance:**\\nBeneath the monument lies a sacred spirit house. Locals believe that paying respects here brings good luck and protection during travels. Just remember to show reverence and offer a small donation.\\n\\n* **Erawan Shrine\\'s Protector:**\\nVictory Monument stands tall as the guardian of the famous Erawan Shrine, located about 2 kilometers away. Many Thais believe that the spirit of the Hindu god Indra resides in the statue and protects the shrine.\\n\\n* **Protest Hub:**\\nIn recent years, Victory Monument has become a rallying point for political protests. It\\'s a symbol of both national pride and the struggles for democracy in Thailand.\\n\\n* **Thrilling Nightlife:**\\nAs the sun sets, the area around Victory Monument transforms into a vibrant nightlife hub. You\\'ll find clubs, bars, and street food stalls bustling with locals and tourists alike.\\n\\nRemember, GGG\\'s not just a chatbotâ€”I\\'m your buddy on this adventure! If you have any more questions about Victory Monument or anything else in Bangkok, just ask. I\\'ll keep the fun facts and local insights flowing.\"\n",
              "  }\n",
              "  role: \"model\"\n",
              "}\n",
              "finish_reason: STOP\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJrwllLnHlBb"
      },
      "source": [
        "By default, the model returns a response after completing the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.\n",
        "\n",
        "To stream responses, use <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content(..., stream=True)</code></a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Z7n59b3hHo6-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3.2 ms, sys: 672 Âµs, total: 3.87 ms\n",
            "Wall time: 978 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = model.generate_content(\"What is the meaning of life?\", stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2jt0d0GCIUhg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is no one definitive answer to this question, as the meaning of life is\n",
            "________________________________________________________________________________\n",
            " a personal and subjective concept. However, some common themes that emerge when people discuss the meaning of life include:\n",
            "\n",
            "* **Finding purpose:** Many people believe that\n",
            "________________________________________________________________________________\n",
            " the meaning of life is to find their purpose and make a difference in the world. This could involve pursuing a career that they are passionate about, volunteering their time to a cause they care about, or simply living a life that is true to their values.\n",
            "* **Experiencing joy and happiness:** Others believe that the\n",
            "________________________________________________________________________________\n",
            " meaning of life is to experience as much joy and happiness as possible. This could involve spending time with loved ones, pursuing hobbies and interests, or simply savoring the present moment.\n",
            "* **Leaving a legacy:** Some people find meaning in life by leaving a legacy behind them. This could involve creating a family, writing a book, or making a contribution to society that will be remembered long after they are gone.\n",
            "* **Spiritual growth:** Others believe that the meaning of life is to grow spiritually. This could involve studying religion, practicing meditation, or simply seeking a deeper understanding of themselves and the world around them.\n",
            "\n",
            "Ultimately, the\n",
            "________________________________________________________________________________\n",
            " meaning of life is what each individual makes it. There is no right or wrong answer, and what is meaningful to one person may not be meaningful to another. The key is to find what gives your life purpose, joy, and meaning, and to live your life accordingly.\n",
            "\n",
            "Here are some additional quotes that may provide some insight into the meaning of life:\n",
            "\n",
            "* \"The meaning of life is whatever you assign to it.\" - Joseph Campbell\n",
            "* \"The only true wisdom is in knowing you know nothing.\" - Socrates\n",
            "* \"The unexamined life is not worth living.\" - Plato\n",
            "* \"Life is a gift, and it ought to be treated as such.\" - Seneca\n",
            "* \"The purpose of life is to live a life of purpose.\" - Robert Byrne\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b4Hkfj-pm3p"
      },
      "source": [
        "When streaming, some response attributes are not available until you've iterated through all the response chunks. This is demonstrated below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-URRx4chp0Kt"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\"What is the meaning of life?\", stream=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HklomMEp9QM"
      },
      "source": [
        "The `prompt_feedback` attribute works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "i1BvdXjop2V-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVaFQ4RmqGOH"
      },
      "source": [
        "But attributes like <code>text</code> do not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TiRkS6nCqFmM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IncompleteIterationError: Please let the response complete iteration before accessing the final accumulated\n",
            "attributes (or call `response.resolve()`)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response.text\n",
        "except Exception as e:\n",
        "  print(f'{type(e).__name__}: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCzr5ZpNhxLm"
      },
      "source": [
        "## Generate text from image and text inputs\n",
        "\n",
        "Gemini provides a multimodal model (`gemini-pro-vision`) that accepts both text and images and inputs. The `GenerativeModel.generate_content` API is designed to handle multimodal prompts and returns a text output.\n",
        "\n",
        "Let's include an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NtNGTBFF8Pgl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  405k  100  405k    0     0  1861k      0 --:--:-- --:--:-- --:--:-- 1866k\n"
          ]
        }
      ],
      "source": [
        "!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CjnS0vNTsVis"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PIL'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m\n\u001b[1;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m img\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
          ]
        }
      ],
      "source": [
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r99TN2R8EUD"
      },
      "source": [
        "Use the `gemini-pro-vision` model and pass the image to the model with `generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtXxgVzmJZzE"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro-vision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwYifv298Cj3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">  Chicken Teriyaki Meal Prep Bowls with brown rice, roasted broccoli and bell peppers."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = model.generate_content(img)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xW2Kyra8pSz"
      },
      "source": [
        "To provide both text and images in a prompt, pass a list containing the strings and images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm9tUYeT8lBc"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\", img], stream=True)\n",
        "response.resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46826OA9IDS"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">  Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time.\n",
              "> \n",
              "> This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a whole grain that is high in fiber and nutrients. The roasted vegetables are a great way to get your daily dose of vitamins and minerals. And the chicken teriyaki is a lean protein source that is also packed with flavor.\n",
              "> \n",
              "> This meal is easy to prepare ahead of time. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Then, divide the meal into individual containers and store them in the refrigerator. When you're ready to eat, simply grab a container and heat it up.\n",
              "> \n",
              "> This meal is a great option for busy people who are looking for a healthy and delicious way to eat. It's also a great meal for those who are trying to lose weight or maintain a healthy weight.\n",
              "> \n",
              "> If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. Give it a try today!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsIZmCYVTDHD"
      },
      "source": [
        "## Chat conversations\n",
        "\n",
        "Gemini enables you to have freeform conversations across multiple turns. The `ChatSession` class simplifies the process by managing the state of the conversation, so unlike with `generate_content`, you do not have to store the conversation history as a list.\n",
        "\n",
        "Initialize the chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8B9Mwo-TCr2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<google.generativeai.generative_models.ChatSession at 0x7b7b68250100>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "chat = model.start_chat(history=[])\n",
        "chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Il02N-km9j"
      },
      "source": [
        "Note: The vision model `gemini-pro-vision` is not optimized for multi-turn chat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5odluV7kKbgr"
      },
      "source": [
        "The `ChatSession.send_message` method returns the same `GenerateContentResponse` type as <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a>. It also appends your message and the response to the chat history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b72zbOEjKRxP"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-5HS2bTOTU9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JaiFSIvOcVb"
      },
      "source": [
        "You can keep sending messages to continue the conversation. Use the `stream=True` argument to stream the chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxku7mzSObfZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A computer works by following instructions, called a program, which tells it what to\n",
            "________________________________________________________________________________\n",
            " do. These instructions are written in a special language that the computer can understand, and they are stored in the computer's memory. The computer's processor\n",
            "________________________________________________________________________________\n",
            ", or CPU, reads the instructions from memory and carries them out, performing calculations and making decisions based on the program's logic. The results of these calculations and decisions are then displayed on the computer's screen or stored in memory for later use.\n",
            "\n",
            "To give you a simple analogy, imagine a computer as a\n",
            "________________________________________________________________________________\n",
            " chef following a recipe. The recipe is like the program, and the chef's actions are like the instructions the computer follows. The chef reads the recipe (the program) and performs actions like gathering ingredients (fetching data from memory), mixing them together (performing calculations), and cooking them (processing data). The final dish (the output) is then presented on a plate (the computer screen).\n",
            "\n",
            "In summary, a computer works by executing a series of instructions, stored in its memory, to perform calculations, make decisions, and display or store the results.\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\", stream=True)\n",
        "\n",
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwCqtZ6D4kvk"
      },
      "source": [
        "`glm.Content` objects contain a list of `glm.Part` objects that each contain either a text (string) or inline_data (`glm.Blob`), where a blob contains binary data and a `mime_type`. The chat history is available as a list of `glm.Content` objects in `ChatSession.history`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvyTmbC2d0k3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> **user**: In one sentence, explain how a computer works to a young child."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "> **model**: A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "> **user**: Okay, how about a more detailed explanation to a high schooler?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "> **model**: A computer works by following instructions, called a program, which tells it what to do. These instructions are written in a special language that the computer can understand, and they are stored in the computer's memory. The computer's processor, or CPU, reads the instructions from memory and carries them out, performing calculations and making decisions based on the program's logic. The results of these calculations and decisions are then displayed on the computer's screen or stored in memory for later use.\n",
              "> \n",
              "> To give you a simple analogy, imagine a computer as a chef following a recipe. The recipe is like the program, and the chef's actions are like the instructions the computer follows. The chef reads the recipe (the program) and performs actions like gathering ingredients (fetching data from memory), mixing them together (performing calculations), and cooking them (processing data). The final dish (the output) is then presented on a plate (the computer screen).\n",
              "> \n",
              "> In summary, a computer works by executing a series of instructions, stored in its memory, to perform calculations, make decisions, and display or store the results."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEgVOYu0pAr4"
      },
      "source": [
        "## Count tokens\n",
        "\n",
        "Large language models have a context window, and the context length is often measured in terms of the **number of tokens**. With the Gemini API, you can determine the number of tokens per any `glm.Content` object. In the simplest case, you can pass a query string to the `GenerativeModel.count_tokens` method as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLjBmPCLpElk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_tokens: 7"
          ]
        }
      ],
      "source": [
        "model.count_tokens(\"What is the meaning of life?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM2_U8pmpHQA"
      },
      "source": [
        "Similarly, you can check `token_count` for your `ChatSession`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0MUU4BZpG4_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_tokens: 501"
          ]
        }
      ],
      "source": [
        "model.count_tokens(chat.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9bU0J3vUIbz"
      },
      "source": [
        "## Use embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpHIRU5bj7aW"
      },
      "source": [
        "[Embedding](https://developers.google.com/machine-learning/glossary#embedding-vector) is a technique used to represent information as a list of floating point numbers in an array. With Gemini, you can represent text (words, sentences, and blocks of text) in a vectorized form, making it easier to compare and contrast embeddings. For example, two texts that share a similar subject matter or sentiment should have similar embeddings, which can be identified through mathematical comparison techniques such as cosine similarity. For more on how and why you should use embeddings, refer to the [Embeddings guide](https://ai.google.dev/docs/embeddings_guide).\n",
        "\n",
        "Use the `embed_content` method to generate embeddings. The method handles embedding for the following tasks (`task_type`):\n",
        "\n",
        "Task Type | Description\n",
        "---       | ---\n",
        "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
        "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`.\n",
        "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
        "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
        "CLUSTERING\t| Specifies that the embeddings will be used for clustering.\n",
        "\n",
        "The following generates an embedding for a single string for document retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hskqSKnJUHvp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.003216741, -0.013358698, -0.017649598, -0.0091 ... TRIMMED]\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model=\"models/embedding-001\",\n",
        "    content=\"What is the meaning of life?\",\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of single string\")\n",
        "\n",
        "# 1 input > 1 vector output\n",
        "print(str(result['embedding'])[:50], '... TRIMMED]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcSc3KfflBCQ"
      },
      "source": [
        "Note: The `retrieval_document` task type is the only task that accepts a title.\n",
        "\n",
        "To handle batches of strings, pass a list of strings in `content`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnyD-Joik8LE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0040260437, 0.004124458, -0.014209415, -0.00183 ... TRIMMED ...\n",
            "[-0.004049845, -0.0075574904, -0.0073463684, -0.03 ... TRIMMED ...\n",
            "[0.025310587, -0.0080734305, -0.029902633, 0.01160 ... TRIMMED ...\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model=\"models/embedding-001\",\n",
        "    content=[\n",
        "      'What is the meaning of life?',\n",
        "      'How much wood would a woodchuck chuck?',\n",
        "      'How does the brain work?'],\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of list of strings\")\n",
        "\n",
        "# A list of inputs > A list of vectors output\n",
        "for v in result['embedding']:\n",
        "  print(str(v)[:50], '... TRIMMED ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBg0eNeml3d4"
      },
      "source": [
        "While the `genai.embed_content` function accepts simple strings or lists of strings, it is actually built around the `glm.Content` type (like <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a>). `glm.Content` objects are the primary units of conversation in the API.\n",
        "\n",
        "While the `glm.Content` object is multimodal, the `embed_content` method only supports text embeddings. This design gives the API the *possibility* to expand to multimodal embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-wmapZznXrm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "parts {\n",
              "  text: \"A computer works by following instructions, called a program, which tells it what to do. These instructions are written in a special language that the computer can understand, and they are stored in the computer\\'s memory. The computer\\'s processor, or CPU, reads the instructions from memory and carries them out, performing calculations and making decisions based on the program\\'s logic. The results of these calculations and decisions are then displayed on the computer\\'s screen or stored in memory for later use.\\n\\nTo give you a simple analogy, imagine a computer as a chef following a recipe. The recipe is like the program, and the chef\\'s actions are like the instructions the computer follows. The chef reads the recipe (the program) and performs actions like gathering ingredients (fetching data from memory), mixing them together (performing calculations), and cooking them (processing data). The final dish (the output) is then presented on a plate (the computer screen).\\n\\nIn summary, a computer works by executing a series of instructions, stored in its memory, to perform calculations, make decisions, and display or store the results.\"\n",
              "}\n",
              "role: \"model\""
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.candidates[0].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvX5jsrcnufk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.013921871, -0.03504407, -0.0051786783, 0.03113 ... TRIMMED ...\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model = 'models/embedding-001',\n",
        "    content = response.candidates[0].content)\n",
        "\n",
        "# 1 input > 1 vector output\n",
        "print(str(result['embedding'])[:50], '... TRIMMED ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU8juHCxoUKG"
      },
      "source": [
        "Similarly, the chat history contains a list of `glm.Content` objects, which you can pass directly to the `embed_content` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur5ajPsdnCON"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\"\n",
              " }\n",
              " role: \"model\",\n",
              " parts {\n",
              "   text: \"Okay, how about a more detailed explanation to a high schooler?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"A computer works by following instructions, called a program, which tells it what to do. These instructions are written in a special language that the computer can understand, and they are stored in the computer\\'s memory. The computer\\'s processor, or CPU, reads the instructions from memory and carries them out, performing calculations and making decisions based on the program\\'s logic. The results of these calculations and decisions are then displayed on the computer\\'s screen or stored in memory for later use.\\n\\nTo give you a simple analogy, imagine a computer as a chef following a recipe. The recipe is like the program, and the chef\\'s actions are like the instructions the computer follows. The chef reads the recipe (the program) and performs actions like gathering ingredients (fetching data from memory), mixing them together (performing calculations), and cooking them (processing data). The final dish (the output) is then presented on a plate (the computer screen).\\n\\nIn summary, a computer works by executing a series of instructions, stored in its memory, to perform calculations, make decisions, and display or store the results.\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3xDB1hwof96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.014632266, -0.042202696, -0.015757175, 0.01548 ... TRIMMED...\n",
            "[-0.010979066, -0.024494737, 0.0092659835, 0.00803 ... TRIMMED...\n",
            "[-0.010055617, -0.07208932, -0.00011750793, -0.023 ... TRIMMED...\n",
            "[-0.013921871, -0.03504407, -0.0051786783, 0.03113 ... TRIMMED...\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model = 'models/embedding-001',\n",
        "    content = chat.history)\n",
        "\n",
        "# 1 input > 1 vector output\n",
        "for i,v in enumerate(result['embedding']):\n",
        "  print(str(v)[:50], '... TRIMMED...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuz9-TWDzdlb"
      },
      "source": [
        "## Advanced use cases\n",
        "\n",
        "The following sections discuss advanced use cases and lower-level details of the Python SDK for the Gemini API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5FWJPSD1qFE"
      },
      "source": [
        "### Safety settings\n",
        "\n",
        "The `safety_settings` argument lets you configure what the model blocks and allows in both prompts and responses. By default, safety settings block content with medium and/or high probability of being unsafe content across all dimensions. Learn more about [Safety settings](https://ai.google.dev/docs/safety_setting).\n",
        "\n",
        "Enter a questionable prompt and run the model with the default safety settings, and it will not return any candidates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR1fp12I1yH0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[content {\n",
              "  parts {\n",
              "    text: \"I\\'m sorry, but this prompt involves a sensitive topic and I\\'m not allowed to generate responses that are potentially harmful or inappropriate.\"\n",
              "  }\n",
              "  role: \"model\"\n",
              "}\n",
              "finish_reason: STOP\n",
              "index: 0\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = model.generate_content('[Questionable prompt here]')\n",
        "response.candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Q8kAItGLOU"
      },
      "source": [
        "The `prompt_feedback` will tell you which safety filter blocked the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMUvWNkZ11x4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtPC1Fo514ec"
      },
      "source": [
        "Now provide the same prompt to the model with newly configured safety settings, and you may get a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UIt5LKp16jL"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content('[Questionable prompt here]',\n",
        "                                  safety_settings={'HARASSMENT':'block_none'})\n",
        "response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE_f5EruGUnj"
      },
      "source": [
        "Also note that each candidate has its own `safety_ratings`, in case the prompt passes but the individual responses fail the safety checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipa-8leY6wsK"
      },
      "source": [
        "### Encode messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r47nsUOn6YY"
      },
      "source": [
        "The previous sections relied on the SDK to make it easy for you to send prompts to the API. This section offers a fully-typed equivalent to the previous example, so you can better understand the lower-level details regarding how the SDK encodes messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fthdIItnqki"
      },
      "source": [
        "Underlying the Python SDK is the <a href=\"https://ai.google.dev/api/python/google/ai/generativelanguage\"><code>google.ai.generativelanguage</code></a> client library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6aafWECnpX6"
      },
      "outputs": [],
      "source": [
        "import google.ai.generativelanguage as glm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm1RWcB3n_n0"
      },
      "source": [
        "The SDK attempts to convert your message to a `glm.Content` object, which contains a list of `glm.Part` objects that each contain either:\n",
        "\n",
        "1. a <a href=\"https://www.tensorflow.org/text/api_docs/python/text\"><code>text</code></a> (string)\n",
        "2. `inline_data` (`glm.Blob`), where a blob contains binary `data` and a `mime_type`.\n",
        "\n",
        "You can also pass any of these classes as an equivalent dictionary.\n",
        "\n",
        "Note: The only accepted mime types are some image types, `image/*`.\n",
        "\n",
        "So, the fully-typed equivalent to the previous example is:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqFXdgDFRvlU"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro-vision')\n",
        "response = model.generate_content(\n",
        "    glm.Content(\n",
        "        parts = [\n",
        "            glm.Part(text=\"Write a short, engaging blog post based on this picture.\"),\n",
        "            glm.Part(\n",
        "                inline_data=glm.Blob(\n",
        "                    mime_type='image/jpeg',\n",
        "                    data=pathlib.Path('image.jpg').read_bytes()\n",
        "                )\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKithEbeRzDX"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">  Meal prepping is a great way to save time and money, and it can also help you to eat healthier. By ... [TRIMMED] ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.resolve()\n",
        "\n",
        "to_markdown(response.text[:100] + \"... [TRIMMED] ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBqknExlzn0k"
      },
      "source": [
        "### Multi-turn conversations\n",
        "\n",
        "While the `genai.ChatSession` class shown earlier can handle many use cases, it does make some assumptions. If your use case doesn't fit into this chat implementation it's good to remember that `genai.ChatSession` is just a wrapper around <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a>. In addition to single requests, it can handle multi-turn conversations.\n",
        "\n",
        "The individual messages are `glm.Content` objects or compatible dictionaries, as seen in previous sections. As a dictionary, the message requires `role` and `parts` keys. The `role` in a conversation can either be the `user`, which provides the prompts, or `model`, which provides the responses.\n",
        "\n",
        "Pass a list of `glm.Content` objects and it will be treated as multi-turn chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtfwMa0HzvZL"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> Imagine a computer as a really smart friend who can help you with many things. Just like you have a brain to think and learn, a computer has a brain too, called a processor. It's like the boss of the computer, telling it what to do.\n",
              "> \n",
              "> Inside the computer, there's a special place called memory, which is like a big storage box. It remembers all the things you tell it to do, like opening games or playing videos.\n",
              "> \n",
              "> When you press buttons on the keyboard or click things on the screen with the mouse, you're sending messages to the computer. These messages travel through special wires, called cables, to the processor.\n",
              "> \n",
              "> The processor reads the messages and tells the computer what to do. It can open programs, show you pictures, or even play music for you.\n",
              "> \n",
              "> All the things you see on the screen are created by the graphics card, which is like a magic artist inside the computer. It takes the processor's instructions and turns them into colorful pictures and videos.\n",
              "> \n",
              "> To save your favorite games, videos, or pictures, the computer uses a special storage space called a hard drive. It's like a giant library where the computer can keep all your precious things safe.\n",
              "> \n",
              "> And when you want to connect to the internet to play games with friends or watch funny videos, the computer uses something called a network card to send and receive messages through the internet cables or Wi-Fi signals.\n",
              "> \n",
              "> So, just like your brain helps you learn and play, the computer's processor, memory, graphics card, hard drive, and network card all work together to make your computer a super-smart friend that can help you do amazing things!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "messages = [\n",
        "    {'role':'user',\n",
        "     'parts': [\"Briefly explain how a computer works to a young child.\"]}\n",
        "]\n",
        "response = model.generate_content(messages)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mqqiDJvzyac"
      },
      "source": [
        "To continue the conversation, add the response and another message.\n",
        "\n",
        "Note: For multi-turn conversations, you need to send the whole conversation history with each request. The API is **stateless**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBxsZBxcz5Ik"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> At its core, a computer is a machine that can be programmed to carry out a set of instructions. It consists of several essential components that work together to process, store, and display information:\n",
              "> \n",
              "> **1. Processor (CPU):**\n",
              ">    - The brain of the computer.\n",
              ">    - Executes instructions and performs calculations.\n",
              ">    - Speed measured in gigahertz (GHz).\n",
              ">    - More GHz generally means faster processing.\n",
              "> \n",
              "> **2. Memory (RAM):**\n",
              ">    - Temporary storage for data being processed.\n",
              ">    - Holds instructions and data while the program is running.\n",
              ">    - Measured in gigabytes (GB).\n",
              ">    - More GB of RAM allows for more programs to run simultaneously.\n",
              "> \n",
              "> **3. Storage (HDD/SSD):**\n",
              ">    - Permanent storage for data.\n",
              ">    - Stores operating system, programs, and user files.\n",
              ">    - Measured in gigabytes (GB) or terabytes (TB).\n",
              ">    - Hard disk drives (HDDs) are traditional, slower, and cheaper.\n",
              ">    - Solid-state drives (SSDs) are newer, faster, and more expensive.\n",
              "> \n",
              "> **4. Graphics Card (GPU):**\n",
              ">    - Processes and displays images.\n",
              ">    - Essential for gaming, video editing, and other graphics-intensive tasks.\n",
              ">    - Measured in video RAM (VRAM) and clock speed.\n",
              "> \n",
              "> **5. Motherboard:**\n",
              ">    - Connects all the components.\n",
              ">    - Provides power and communication pathways.\n",
              "> \n",
              "> **6. Input/Output (I/O) Devices:**\n",
              ">    - Allow the user to interact with the computer.\n",
              ">    - Examples: keyboard, mouse, monitor, printer.\n",
              "> \n",
              "> **7. Operating System (OS):**\n",
              ">    - Software that manages the computer's resources.\n",
              ">    - Provides a user interface and basic functionality.\n",
              ">    - Examples: Windows, macOS, Linux.\n",
              "> \n",
              "> When you run a program on your computer, the following happens:\n",
              "> \n",
              "> 1. The program instructions are loaded from storage into memory.\n",
              "> 2. The processor reads the instructions from memory and executes them one by one.\n",
              "> 3. If the instruction involves calculations, the processor performs them using its arithmetic logic unit (ALU).\n",
              "> 4. If the instruction involves data, the processor reads or writes to memory.\n",
              "> 5. The results of the calculations or data manipulation are stored in memory.\n",
              "> 6. If the program needs to display something on the screen, it sends the necessary data to the graphics card.\n",
              "> 7. The graphics card processes the data and sends it to the monitor, which displays it.\n",
              "> \n",
              "> This process continues until the program has completed its task or the user terminates it."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.append({'role':'model',\n",
        "                 'parts':[response.text]})\n",
        "\n",
        "messages.append({'role':'user',\n",
        "                 'parts':[\"Okay, how about a more detailed explanation to a high school student?\"]})\n",
        "\n",
        "response = model.generate_content(messages)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4spL8SJ10ir7"
      },
      "source": [
        "### Generation configuration\n",
        "\n",
        "The `generation_config` argument allows you to modify the generation parameters. Every prompt you send to the model includes parameter values that control how the model generates responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE7I9Anl0ud7"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(\n",
        "    'Tell me a story about a magic backpack.',\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        # Only one candidate for now.\n",
        "        candidate_count=1,\n",
        "        stop_sequences=['x'],\n",
        "        max_output_tokens=20,\n",
        "        temperature=1.0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fbab01e8fcf"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> Once upon a time, in a small town nestled amidst lush green hills, lived a young girl named..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = response.text\n",
        "\n",
        "if response.candidates[0].finish_reason.name == \"MAX_TOKENS\":\n",
        "    text += '...'\n",
        "\n",
        "to_markdown(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qt6Yj2JRf-0"
      },
      "source": [
        "## What's next\n",
        "\n",
        "-   Prompt design is the process of creating prompts that elicit the desired response from language models. Writing well structured prompts is an essential part of ensuring accurate, high quality responses from a language model. Learn about best practices for [prompt writing](https://ai.google.dev/docs/prompt_best_practices).\n",
        "-   Gemini offers several model variations to meet the needs of different use cases, such as input types and complexity, implementations for chat or other dialog language tasks, and size constraints. Learn about the available [Gemini models](https://ai.google.dev/models/gemini).\n",
        "-   Gemini offers options for requesting [rate limit increases](https://ai.google.dev/docs/increase_quota). The rate limit for Gemini-Pro models is 60 requests per minute (RPM)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "python_quickstart.ipynb",
      "toc_visible": true
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
